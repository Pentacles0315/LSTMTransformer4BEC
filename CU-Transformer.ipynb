{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad7a02a-b046-41aa-b4c4-e8ccf0ab7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from utils import *\n",
    "from test import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d6babbb-d255-4b3c-933b-a0174ad2177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = CU_BEMS()\n",
    "X_columns_to_normalize = X.columns.difference(['date'])\n",
    "X_scaler = MinMaxScaler()\n",
    "X[X_columns_to_normalize] = X_scaler.fit_transform(X[X_columns_to_normalize])\n",
    "\n",
    "X_train = X[X[\"date\"] <= '2019-10-01']\n",
    "y_train = y[y[\"date\"] <= '2019-10-01']\n",
    "\n",
    "X_test = X[X[\"date\"] >= '2019-10-01']\n",
    "y_test = y[y[\"date\"] >= '2019-10-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466a6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "predict_length = 1\n",
    "batch_size = 100\n",
    "\n",
    "Train_dataset = TimeSeriesDataset_sep(X_train, y_train, seq_length, predict_length = predict_length)\n",
    "Train_dataloader = DataLoader(Train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "Test_dataset = TimeSeriesDataset_sep(X_test, y_test, seq_length, predict_length = predict_length)\n",
    "Test_dataloader = DataLoader(Test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a7f86d4-697e-4ce6-9f0b-e034a29671d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\capstone\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "input_size = len(X.columns)\n",
    "output_size = 1\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "num_heads = 5\n",
    "model = BiLSTMTransformer(input_size, hidden_size, num_layers, output_size, num_heads, predict_length).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e14ed8e-0b2e-4d4c-bcf7-1ac3735cc8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1c29d9d-25f4-4479-ba51-59ce2a3c2d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 46655.9336, Val Loss: 28623.4837, Val MSE: 28629.4023, Val MAE: 152.0427, Val R²: -0.0932\n",
      "Epoch [2/50], Loss: 46774.6523, Val Loss: 29418.5055, Val MSE: 29407.7188, Val MAE: 156.0596, Val R²: -0.1229\n",
      "Epoch [3/50], Loss: 46637.2383, Val Loss: 28976.4618, Val MSE: 28974.6855, Val MAE: 153.8848, Val R²: -0.1064\n",
      "Epoch [4/50], Loss: 46616.5664, Val Loss: 29624.9703, Val MSE: 29610.1777, Val MAE: 157.0310, Val R²: -0.1307\n",
      "Epoch [5/50], Loss: 47030.9062, Val Loss: 29872.2673, Val MSE: 29852.8340, Val MAE: 158.1595, Val R²: -0.1399\n",
      "Epoch [6/50], Loss: 46185.8086, Val Loss: 27444.4499, Val MSE: 27481.0098, Val MAE: 144.8307, Val R²: -0.0494\n",
      "Epoch [7/50], Loss: 46788.0977, Val Loss: 29544.8845, Val MSE: 29531.6328, Val MAE: 156.6573, Val R²: -0.1277\n",
      "Epoch [8/50], Loss: 47205.6328, Val Loss: 30229.7971, Val MSE: 30203.9102, Val MAE: 159.7300, Val R²: -0.1533\n",
      "Epoch [9/50], Loss: 47049.4375, Val Loss: 30375.2880, Val MSE: 30346.8555, Val MAE: 160.3519, Val R²: -0.1588\n",
      "Epoch [10/50], Loss: 47272.2188, Val Loss: 30464.3076, Val MSE: 30434.3398, Val MAE: 160.7283, Val R²: -0.1621\n",
      "Epoch [11/50], Loss: 47102.7461, Val Loss: 30454.5909, Val MSE: 30424.7891, Val MAE: 160.6874, Val R²: -0.1618\n",
      "Epoch [12/50], Loss: 47087.3320, Val Loss: 30484.1523, Val MSE: 30453.8438, Val MAE: 160.8117, Val R²: -0.1629\n",
      "Epoch [13/50], Loss: 47324.6562, Val Loss: 30337.2027, Val MSE: 30309.4336, Val MAE: 160.1900, Val R²: -0.1574\n",
      "Epoch [14/50], Loss: 46576.6367, Val Loss: 28749.6697, Val MSE: 28752.7773, Val MAE: 152.7145, Val R²: -0.0979\n",
      "Epoch [15/50], Loss: 46384.0352, Val Loss: 28827.8540, Val MSE: 28829.2539, Val MAE: 153.1226, Val R²: -0.1008\n",
      "Epoch [16/50], Loss: 46608.6562, Val Loss: 28822.9531, Val MSE: 28824.4570, Val MAE: 153.0971, Val R²: -0.1007\n",
      "Epoch [17/50], Loss: 46541.0391, Val Loss: 28827.7959, Val MSE: 28829.1992, Val MAE: 153.1223, Val R²: -0.1008\n",
      "Epoch [18/50], Loss: 46343.4688, Val Loss: 28827.5551, Val MSE: 28828.9609, Val MAE: 153.1210, Val R²: -0.1008\n",
      "Epoch [19/50], Loss: 46414.2891, Val Loss: 28827.9279, Val MSE: 28829.3281, Val MAE: 153.1229, Val R²: -0.1009\n",
      "Epoch [20/50], Loss: 46416.0664, Val Loss: 28827.0362, Val MSE: 28828.4551, Val MAE: 153.1183, Val R²: -0.1008\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# 反向传播和优化\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 33\u001b[0m     total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     34\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#scheduler.step()\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\capstone\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    523\u001b[0m )\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\capstone\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[0;32m    290\u001b[0m     tensors,\n\u001b[0;32m    291\u001b[0m     grad_tensors_,\n\u001b[0;32m    292\u001b[0m     retain_graph,\n\u001b[0;32m    293\u001b[0m     create_graph,\n\u001b[0;32m    294\u001b[0m     inputs,\n\u001b[0;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m )\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\capstone\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.8\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.SmoothL1Loss()  # 用于回归任务\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "loss_per_epoch = []\n",
    "val_mse_per_epoch = []\n",
    "val_r2_per_epoch = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 确保模型在训练模式下\n",
    "    for external, internal, batch_y in Train_dataloader:\n",
    "        external, internal, batch_y = external.to(device), internal.to(device), batch_y.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        total_loss = 0\n",
    "        y = internal[:, -1:, :]\n",
    "        for step in range(predict_length):\n",
    "            outputs = model(external, internal, y)\n",
    "            next_pred = outputs[:, -1:, :]\n",
    "            #outputs = model(batch_X, batch_y.view(batch_y.shape[0], batch_y.shape[2]))\n",
    "            loss = criterion(next_pred, batch_y[:, step:step+1, :])\n",
    "            total_loss += loss\n",
    "            if np.random.rand() < teacher_forcing_ratio:\n",
    "                next_input = batch_y[:, step:step + 1, :]\n",
    "            else:\n",
    "                next_input = next_pred\n",
    "            \n",
    "            y = torch.cat([y, next_input], dim=1)\n",
    "\n",
    "        total_loss = total_loss / predict_length\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #scheduler.step()\n",
    "\n",
    "    loss_per_epoch.append(loss.item())\n",
    "    teacher_forcing_ratio -= 0.05\n",
    "\n",
    "    # 评估验证集\n",
    "    val_loss, (val_mse, val_mae, val_r2) = evaluate_Transformer(model, Test_dataloader, criterion, device, [mean_squared_error, mean_absolute_error, r2_score])\n",
    "    val_mse_per_epoch.append(val_mse)\n",
    "    val_r2_per_epoch.append(val_r2)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Val MSE: {val_mse:.4f}, Val MAE: {val_mae:.4f}, Val R²: {val_r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91354a5-3a43-44e9-abd5-f1fde0213659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
