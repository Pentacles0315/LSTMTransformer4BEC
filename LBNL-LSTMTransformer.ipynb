{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf36ab4-6655-47e1-883c-49f520a7ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from utils_function import *\n",
    "from test_function import *\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "933e3d4c-97ab-4649-9cd1-26340a70a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = LBNL59()\n",
    "X_columns_to_normalize = X.columns.difference(['date'])\n",
    "X_scaler = MinMaxScaler()\n",
    "X[X_columns_to_normalize] = X_scaler.fit_transform(X[X_columns_to_normalize])\n",
    "y_columns_to_sum = y.columns.difference(['date'])\n",
    "y['sum'] = y[y_columns_to_sum].sum(axis=1)\n",
    "y = y[[\"date\", 'sum']]\n",
    "X = X[X[\"date\"] >= '2018-09-16']\n",
    "y = y[y[\"date\"] >= '2018-09-16']\n",
    "\n",
    "X_train = X[X[\"date\"] <= '2020-10-16']\n",
    "y_train = y[y[\"date\"] <= '2020-10-16']\n",
    "\n",
    "X_test = X[X[\"date\"] >= '2020-10-16']\n",
    "y_test = y[y[\"date\"] >= '2020-10-16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2731f4d0-2a19-4671-9400-b0ff8b05a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "predict_length = 1\n",
    "batch_size = 100\n",
    "\n",
    "Train_dataset = TimeSeriesDataset_sep(X_train, y_train, seq_length, predict_length = predict_length)\n",
    "Train_dataloader = DataLoader(Train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "Test_dataset = TimeSeriesDataset_sep(X_test, y_test, seq_length, predict_length = predict_length)\n",
    "Test_dataloader = DataLoader(Test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37902c98-0147-4a9e-9a0e-b80442b7242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "input_size = 306\n",
    "output_size = 1\n",
    "hidden_size = 250\n",
    "num_layers = 2\n",
    "num_heads = 10\n",
    "model = BiLSTMTransformer(input_size, hidden_size, num_layers, output_size, num_heads, predict_length).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9619fe52-2fd8-4bf5-9272-a38c0e2a8010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1991.8296, Val Loss: 323.7746, Val MSE: 324.1068, Val MAE: 15.6812, Val R²: -1.0721, Val MAPE: 0.5207\n",
      "Epoch [2/50], Loss: 1300.6281, Val Loss: 170.8737, Val MSE: 172.2729, Val MAE: 11.6555, Val R²: -0.1014, Val MAPE: 0.5561\n",
      "Epoch [3/50], Loss: 813.9973, Val Loss: 171.0255, Val MSE: 173.3895, Val MAE: 9.4908, Val R²: -0.1085, Val MAPE: 0.6429\n",
      "Epoch [4/50], Loss: 479.6570, Val Loss: 26.3894, Val MSE: 26.6823, Val MAE: 3.2613, Val R²: 0.8294, Val MAPE: 0.1583\n",
      "Epoch [5/50], Loss: 272.1000, Val Loss: 18.8804, Val MSE: 19.0759, Val MAE: 2.8538, Val R²: 0.8780, Val MAPE: 0.1472\n",
      "Epoch [6/50], Loss: 154.6667, Val Loss: 18.5413, Val MSE: 18.6672, Val MAE: 3.0937, Val R²: 0.8807, Val MAPE: 0.1356\n",
      "Epoch [7/50], Loss: 107.2104, Val Loss: 17.9840, Val MSE: 18.1430, Val MAE: 2.9674, Val R²: 0.8840, Val MAPE: 0.1430\n",
      "Epoch [8/50], Loss: 56.7616, Val Loss: 15.0907, Val MSE: 15.2345, Val MAE: 2.5865, Val R²: 0.9026, Val MAPE: 0.1307\n",
      "Epoch [9/50], Loss: 45.4315, Val Loss: 14.8285, Val MSE: 14.9657, Val MAE: 2.5700, Val R²: 0.9043, Val MAPE: 0.1271\n",
      "Epoch [10/50], Loss: 31.9169, Val Loss: 15.4440, Val MSE: 15.5759, Val MAE: 2.7350, Val R²: 0.9004, Val MAPE: 0.1438\n",
      "Epoch [11/50], Loss: 33.3538, Val Loss: 14.8204, Val MSE: 14.9426, Val MAE: 2.6356, Val R²: 0.9045, Val MAPE: 0.1353\n",
      "Epoch [12/50], Loss: 27.1761, Val Loss: 14.2515, Val MSE: 14.3706, Val MAE: 2.5221, Val R²: 0.9081, Val MAPE: 0.1243\n",
      "Epoch [13/50], Loss: 27.8711, Val Loss: 15.3661, Val MSE: 15.4793, Val MAE: 2.7635, Val R²: 0.9010, Val MAPE: 0.1378\n",
      "Epoch [14/50], Loss: 26.8037, Val Loss: 15.2183, Val MSE: 15.3312, Val MAE: 2.7614, Val R²: 0.9020, Val MAPE: 0.1422\n",
      "Epoch [15/50], Loss: 26.7104, Val Loss: 14.9450, Val MSE: 15.0520, Val MAE: 2.7397, Val R²: 0.9038, Val MAPE: 0.1415\n",
      "Epoch [16/50], Loss: 28.2003, Val Loss: 14.8242, Val MSE: 14.9333, Val MAE: 2.7193, Val R²: 0.9045, Val MAPE: 0.1440\n",
      "Epoch [17/50], Loss: 29.5448, Val Loss: 12.7162, Val MSE: 12.8161, Val MAE: 2.2921, Val R²: 0.9181, Val MAPE: 0.1129\n",
      "Epoch [18/50], Loss: 32.0749, Val Loss: 12.8268, Val MSE: 12.9374, Val MAE: 2.3216, Val R²: 0.9173, Val MAPE: 0.1176\n",
      "Epoch [19/50], Loss: 30.2298, Val Loss: 12.6106, Val MSE: 12.7158, Val MAE: 2.2894, Val R²: 0.9187, Val MAPE: 0.1146\n",
      "Epoch [20/50], Loss: 31.7425, Val Loss: 12.7103, Val MSE: 12.8204, Val MAE: 2.3145, Val R²: 0.9180, Val MAPE: 0.1177\n",
      "Epoch [21/50], Loss: 31.9299, Val Loss: 12.9762, Val MSE: 13.0901, Val MAE: 2.3751, Val R²: 0.9163, Val MAPE: 0.1242\n",
      "Epoch [22/50], Loss: 31.9038, Val Loss: 12.6361, Val MSE: 12.7503, Val MAE: 2.3088, Val R²: 0.9185, Val MAPE: 0.1171\n",
      "Epoch [23/50], Loss: 33.5953, Val Loss: 12.3343, Val MSE: 12.4430, Val MAE: 2.2306, Val R²: 0.9204, Val MAPE: 0.1078\n",
      "Epoch [24/50], Loss: 32.9742, Val Loss: 12.8045, Val MSE: 12.9204, Val MAE: 2.3437, Val R²: 0.9174, Val MAPE: 0.1206\n",
      "Epoch [25/50], Loss: 33.7043, Val Loss: 12.6497, Val MSE: 12.7636, Val MAE: 2.3294, Val R²: 0.9184, Val MAPE: 0.1160\n",
      "Epoch [26/50], Loss: 32.5688, Val Loss: 12.7344, Val MSE: 12.8476, Val MAE: 2.3494, Val R²: 0.9179, Val MAPE: 0.1161\n",
      "Epoch [27/50], Loss: 32.4342, Val Loss: 12.5722, Val MSE: 12.6828, Val MAE: 2.3098, Val R²: 0.9189, Val MAPE: 0.1109\n",
      "Epoch [28/50], Loss: 31.8343, Val Loss: 12.5390, Val MSE: 12.6501, Val MAE: 2.3148, Val R²: 0.9191, Val MAPE: 0.1120\n",
      "Epoch [29/50], Loss: 33.2625, Val Loss: 12.7686, Val MSE: 12.8816, Val MAE: 2.3554, Val R²: 0.9176, Val MAPE: 0.1145\n",
      "Epoch [30/50], Loss: 31.6580, Val Loss: 12.8409, Val MSE: 12.9535, Val MAE: 2.4019, Val R²: 0.9172, Val MAPE: 0.1177\n",
      "Epoch [31/50], Loss: 31.8577, Val Loss: 12.3947, Val MSE: 12.5032, Val MAE: 2.2322, Val R²: 0.9201, Val MAPE: 0.1029\n",
      "Epoch [32/50], Loss: 33.9429, Val Loss: 12.5550, Val MSE: 12.6647, Val MAE: 2.3056, Val R²: 0.9190, Val MAPE: 0.1081\n",
      "Epoch [33/50], Loss: 29.9660, Val Loss: 12.7704, Val MSE: 12.8833, Val MAE: 2.3816, Val R²: 0.9176, Val MAPE: 0.1168\n",
      "Epoch [34/50], Loss: 32.2204, Val Loss: 12.7802, Val MSE: 12.8913, Val MAE: 2.3718, Val R²: 0.9176, Val MAPE: 0.1119\n",
      "Epoch [35/50], Loss: 32.1693, Val Loss: 12.8758, Val MSE: 12.9862, Val MAE: 2.3475, Val R²: 0.9170, Val MAPE: 0.1084\n",
      "Epoch [36/50], Loss: 30.9709, Val Loss: 12.6206, Val MSE: 12.7309, Val MAE: 2.3228, Val R²: 0.9186, Val MAPE: 0.1103\n",
      "Epoch [37/50], Loss: 32.9241, Val Loss: 12.5420, Val MSE: 12.6506, Val MAE: 2.2649, Val R²: 0.9191, Val MAPE: 0.1031\n",
      "Epoch [38/50], Loss: 33.0736, Val Loss: 12.7365, Val MSE: 12.8470, Val MAE: 2.3247, Val R²: 0.9179, Val MAPE: 0.1103\n",
      "Epoch [39/50], Loss: 31.1784, Val Loss: 12.4677, Val MSE: 12.5751, Val MAE: 2.2143, Val R²: 0.9196, Val MAPE: 0.0999\n",
      "Epoch [40/50], Loss: 31.2543, Val Loss: 12.7183, Val MSE: 12.8260, Val MAE: 2.2381, Val R²: 0.9180, Val MAPE: 0.0984\n",
      "Epoch [41/50], Loss: 29.4303, Val Loss: 12.5717, Val MSE: 12.6819, Val MAE: 2.2219, Val R²: 0.9189, Val MAPE: 0.1011\n",
      "Epoch [42/50], Loss: 29.9799, Val Loss: 12.7678, Val MSE: 12.8803, Val MAE: 2.3401, Val R²: 0.9177, Val MAPE: 0.1143\n",
      "Epoch [43/50], Loss: 30.2734, Val Loss: 12.4925, Val MSE: 12.5975, Val MAE: 2.1614, Val R²: 0.9195, Val MAPE: 0.0917\n",
      "Epoch [44/50], Loss: 27.0284, Val Loss: 12.5571, Val MSE: 12.6618, Val MAE: 2.1797, Val R²: 0.9191, Val MAPE: 0.0931\n",
      "Epoch [45/50], Loss: 29.1309, Val Loss: 12.5953, Val MSE: 12.7028, Val MAE: 2.1997, Val R²: 0.9188, Val MAPE: 0.0948\n",
      "Epoch [46/50], Loss: 27.8288, Val Loss: 12.5231, Val MSE: 12.6273, Val MAE: 2.1948, Val R²: 0.9193, Val MAPE: 0.0964\n",
      "Epoch [47/50], Loss: 29.3898, Val Loss: 12.5796, Val MSE: 12.6841, Val MAE: 2.1732, Val R²: 0.9189, Val MAPE: 0.0940\n",
      "Epoch [48/50], Loss: 27.7498, Val Loss: 12.6468, Val MSE: 12.7509, Val MAE: 2.1606, Val R²: 0.9185, Val MAPE: 0.0933\n",
      "Epoch [49/50], Loss: 25.9646, Val Loss: 12.4618, Val MSE: 12.5632, Val MAE: 2.1607, Val R²: 0.9197, Val MAPE: 0.0936\n",
      "Epoch [50/50], Loss: 25.5650, Val Loss: 12.5993, Val MSE: 12.6943, Val MAE: 2.2028, Val R²: 0.9188, Val MAPE: 0.0980\n"
     ]
    }
   ],
   "source": [
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "teacher_forcing_ratio = 0\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.SmoothL1Loss()  # 用于回归任务\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "loss_per_epoch = []\n",
    "val_mse_per_epoch = []\n",
    "val_r2_per_epoch = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 确保模型在训练模式下\n",
    "    for external, internal, batch_y in Train_dataloader:\n",
    "        external, internal, batch_y = external.to(device), internal.to(device), batch_y.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        total_loss = 0\n",
    "        y = internal[:, -1:, :]\n",
    "        for step in range(predict_length):\n",
    "            outputs = model(external, internal, y)\n",
    "            next_pred = outputs[:, -1:, :]\n",
    "            #outputs = model(batch_X, batch_y.view(batch_y.shape[0], batch_y.shape[2]))\n",
    "            loss = criterion(next_pred, batch_y[:, step:step+1, :])\n",
    "            total_loss += loss\n",
    "            if np.random.rand() < teacher_forcing_ratio:\n",
    "                next_input = batch_y[:, step:step + 1, :]\n",
    "            else:\n",
    "                next_input = next_pred\n",
    "            y = torch.cat([y, next_input], dim=1)\n",
    "                \n",
    "        total_loss = total_loss / predict_length\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #scheduler.step()\n",
    "\n",
    "    loss_per_epoch.append(loss.item())\n",
    "    teacher_forcing_ratio -= 0.05\n",
    "\n",
    "    # 评估验证集\n",
    "    val_loss, (val_mse, val_mae, val_r2, val_mape) = evaluate_Transformer(model, Test_dataloader, criterion, device, [mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error])\n",
    "    val_mse_per_epoch.append(val_mse)\n",
    "    val_r2_per_epoch.append(val_r2)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Val MSE: {val_mse:.4f}, Val MAE: {val_mae:.4f}, Val R²: {val_r2:.4f}, Val MAPE: {val_mape:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b7e529-a797-4c68-94a7-7bce64a499ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
