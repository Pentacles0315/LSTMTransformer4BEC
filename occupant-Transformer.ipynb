{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ecb82b-e4c5-4f0e-beb9-9f4f7dba7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from utils import *\n",
    "from test import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6169ed8e-caf5-4e23-9878-262b37ed0e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = occupant()\n",
    "X_columns_to_normalize = X.columns.difference([\"timestamp [dd/mm/yyyy HH:MM]\"])\n",
    "X_scaler = MinMaxScaler()\n",
    "X[X_columns_to_normalize] = X_scaler.fit_transform(X[X_columns_to_normalize])\n",
    "\n",
    "X_train = X[X[\"timestamp [dd/mm/yyyy HH:MM]\"] <= '2013-09-30']\n",
    "y_train = y[y[\"timestamp [dd/mm/yyyy HH:MM]\"] <= '2013-09-30']\n",
    "\n",
    "X_test = X[X[\"timestamp [dd/mm/yyyy HH:MM]\"] >= '2013-09-30']\n",
    "y_test = y[y[\"timestamp [dd/mm/yyyy HH:MM]\"] >= '2013-09-30']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c4d507-9973-44b1-bb2d-170d26e75d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "predict_length = 5\n",
    "batch_size = 100\n",
    "\n",
    "Train_dataset = TimeSeriesDataset_sep(X_train, y_train, seq_length, predict_length = predict_length, time = \"timestamp [dd/mm/yyyy HH:MM]\")\n",
    "Train_dataloader = DataLoader(Train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "Test_dataset = TimeSeriesDataset_sep(X_test, y_test, seq_length, predict_length = predict_length, time = \"timestamp [dd/mm/yyyy HH:MM]\")\n",
    "Test_dataloader = DataLoader(Test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb83d69-7749-4bca-a6bf-056c88515db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "input_size = 43\n",
    "output_size = 1\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "num_heads = 5\n",
    "model = BiLSTMTransformer(input_size, hidden_size, num_layers, output_size, num_heads, predict_length).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c2b58-585c-45a9-928c-99240cde6444",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 5\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.SmoothL1Loss()  # 用于回归任务\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "loss_per_epoch = []\n",
    "val_mse_per_epoch = []\n",
    "val_r2_per_epoch = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 确保模型在训练模式下\n",
    "    for external, internal, batch_y in Train_dataloader:\n",
    "        external, internal, batch_y = external.to(device), internal.to(device), batch_y.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        total_loss = 0\n",
    "        y = internal[:, -1:, :]\n",
    "        for step in range(predict_length):\n",
    "            outputs = model(external, internal, y)\n",
    "            next_pred = outputs[:, -1:, :]\n",
    "            #outputs = model(batch_X, batch_y.view(batch_y.shape[0], batch_y.shape[2]))\n",
    "            loss = criterion(next_pred, batch_y[:, step:step+1, :])\n",
    "            total_loss += loss\n",
    "            if np.random.rand() < teacher_forcing_ratio:\n",
    "                next_input = batch_y[:, step:step + 1, :]\n",
    "            else:\n",
    "                next_input = next_pred\n",
    "            y = torch.cat([y, next_input], dim=1)\n",
    "                \n",
    "        total_loss = total_loss / predict_length\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #scheduler.step()\n",
    "\n",
    "    loss_per_epoch.append(loss.item())\n",
    "    teacher_forcing_ratio -= 0.05\n",
    "\n",
    "    # 评估验证集\n",
    "    val_loss, (val_mse, val_mae, val_r2) = evaluate_Transformer(model, Test_dataloader, criterion, device, [mean_squared_error, mean_absolute_error, r2_score])\n",
    "    val_mse_per_epoch.append(val_mse)\n",
    "    val_r2_per_epoch.append(val_r2)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item():.4f}, Val Loss: {val_loss:.4f}, Val MSE: {val_mse:.4f}, Val R²: {val_r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1582e6-f337-4f2d-89d9-ea531d657d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
